<h1>科技公司在商量如何规范人工智能，它的伦理该是什么？_智能_好奇心日报
</h1><p><img src="http://img.qdaily.com/article/article_show/20160903083638cnw6EL8HsuqkOyfr.jpeg?imageMogr2/auto-orient/thumbnail/!755x450r/gravity/Center/crop/755x450/ignore-error/1"></p><div class="article-detail-bd"><div class="author-share clearfix">   <div class="author">  <a rel="nofollow" href="javascript:void(0)" class="avatar x32 circle"><img src="http://m.qdaily.com/images/missing_face.png"/> </a>  <span class="name">John Markoff</span><span class="date smart-date" data-origindate="2016-09-04 08:46:05 +0800"/><script language="JavaScript" type="text/javascript"><![CDATA[var o="div",a=" style='disp",b="lay:",c="none'";document.write("<"+o+a+b+c+">")]]></script><span class="date">2016-09-04 08:46:05</span><script language="JavaScript" type="text/javascript"><![CDATA[var o="div";document.write("</"+o+">")]]></script></div>      <div class="com-share-favor" data-id="31776" data-title="《科技公司在商量如何规范人工智能，它的伦理该是什么？》，来自@好奇心日报" data-pic="http://img.qdaily.com/article/article_show/20160903083638cnw6EL8HsuqkOyfr.jpeg?imageMogr2/auto-orient/thumbnail/!640x380r/gravity/Center/crop/640x380/ignore-error/1" data-url="http://www.qdaily.com/articles/31776.html" data-weiboappkey="2462590045"><div class="share-favor-bd clearfix"><a rel="nofollow" data-ga-event="pc:share:weixin" href="javascript:void(0)" class="share iconfont icon-weixin"/><a rel="nofollow" data-ga-event="pc:share:weibo" href="http://service.weibo.com/share/share.php" class="share iconfont icon-weibo"/><a rel="nofollow" data-ga-event="pc:share:tengxunweixin" href="http://share.v.t.qq.com/index.php" class="share iconfont icon-tengxunweibo"/><a rel="nofollow" data-ga-event="pc:share:kongjian" href="http://sns.qzone.qq.com/cgi-bin/qzshare/cgi_qzshare_onekey" class="share iconfont icon-kongjian"/><a rel="nofollow" data-ga-event="pc:share:douban" href="http://www.douban.com/share/service" class="share iconfont icon-douban"/><a rel="nofollow" data-ga-event="pc:share:linkedin" href="http://www.linkedin.com/shareArticle" class="share iconfont icon-linkedin"/>  <a rel="nofollow" data-ga-event="pc:favor:aritcle" href="#" class="favor iconfont icon-heart"><span class="num  smart-count" data-origincount="94"/></a>  </div></div> </div>  <p class="excerpt">就业、运输、战争，这些地方机器人会影响人类什么？</p>  <div class="detail">  

<p><b><i>＊</i><i>本文</i><i>只能在《好奇心日报》发布，即使我们允许了也不许转载＊</i></b></p>
<p>旧金山电 — 多年来，科幻电影制作人反复强调人工智能机器可能对他们的人类制造者做出什么坏事，令人心生恐惧。但在接下来的一二十年，我们最关心的更有可能是机器人会不会抢走我们的饭碗，或是在公路上撞到我们。</p>
<p>现在，全球五大科技巨头正试图建立人工智能创造过程中的一套伦理规范。科幻小说关注的多是人工智能对人类生存的威胁，而来自 Google 母公司 <a href="http://www.nytimes.com/topic/company/alphabet-inc?inline=nyt-org" style="background-color: rgb(255, 255, 255);" rel="nofollow">Alphabet</a> 以及<a href="http://www.nytimes.com/topic/company/amazoncom-inc?inline=nyt-org" style="background-color: rgb(255, 255, 255);" rel="nofollow">亚马逊</a>、<a href="http://www.nytimes.com/topic/company/facebook-inc?inline=nyt-org" style="background-color: rgb(255, 255, 255);" rel="nofollow">Facebook</a>、<a href="http://www.nytimes.com/topic/company/international-business-machines-corporation?inline=nyt-org" style="background-color: rgb(255, 255, 255);" rel="nofollow">IBM</a> 和<a href="http://www.nytimes.com/topic/company/microsoft-corporation?inline=nyt-org" style="background-color: rgb(255, 255, 255);" rel="nofollow">微</a><a href="http://www.nytimes.com/topic/company/microsoft-corporation?inline=nyt-org" style="background-color: rgb(255, 255, 255);" rel="nofollow">软</a>的研究员聚在一起讨论的则是更实在的问题，比如人工智能对就业、运输甚至战争的影响。</p>
<div class="com-insert-images"><figure style="margin:0;">&#13;
    <img alt="" data-ratio="0.666667" data-format="jpeg" class="lazyload" src="http://img.qdaily.com/uploads/201609030837534aIEC7XDL1RmkoGq.jpg-w600"/>&#13;
<figcaption class="">二月，Facebook 总裁马克·扎克伯格出席颁奖典礼。Facebook、亚马逊、IBM 等公司研究员汇聚一堂，讨论人工智能对就业、运输甚至战争的影响。图片版权：Kay Nietfeld / 法新社 — 盖蒂图片社</figcaption></figure></div>
<p>长期以来，科技公司对人工智能机器的用途夸下海口，却少有兑现。然而近几年，从自动驾驶车和理解对话的机器（比如亚马逊的 Echo 智能音箱），到可以进行自动化作战的新一代武器系统，人工智能在诸多领域取得了飞速的进展。</p>
<p>这一行业集团具体会怎么做或怎么说，甚至是这套规范的名字都尚未敲定。但它基本的意图很明显：根据参与这一行业伙伴关系建立、但未被授权公开谈论此事的四位人士透露，他们是要确保人工智能研究专注于造福人类，而非危害人类。</p>
<p>这一行业努力的重要性，在周四发布的<a href="http://ai100.stanford.edu/2016-report" rel="nofollow">一份报告</a>中得到了进一步强调。报告出自斯坦福大学一个研究团体，该团体由微软研究员埃里克·霍维茨（Eric Horvitz）资助。埃里克同时也是出席这次行业大讨论的高管之一。这个名为“人工智能百年研究”（One Hundred Year Study on Artificial Intelligence）的斯坦福项目制定了一项计划，试图在接下来的一百年里每五年制作一份详细的报告，论述人工智能对社会的影响。</p>
<p><img data-format="jpg" data-ratio="0.410096" class="lazyloa lazyload" alt="" src="http://img.qdaily.com/uploads/20160918124612v5wnh60OmgCAUPGJ.jpg-w600"/></p>
<p>科技行业人士关心的主要问题之一，大概是是否需要有监管者介入其中，制定人工智能工作的法则。于是他们试图建立一个自律性组织的框架，不过目前还不清楚其将如何运转。</p>
<p>斯坦福报告的作者之一、得克萨斯大学奥斯汀分校计算机科学家彼得·斯通（Peter Stone）说：“我们不是说不用定什么规矩。我们是说如何定是有对错之分的。”</p>
<p>尽管众所周知，科技行业竞争激烈，然而在一些时候，如果能实现利益最大化，公司之间还是可以携手合作的。比如 1990 年代，各科技公司就电子商务交易加密标准达成共识，为接下来二十年里互联网业务的突飞猛进打下了基础。</p>
<p>这份斯坦福报告名为《2030 年人工智能及生活展望》（Artificial Intelligence and Life in 2030）。报告作者主张，规范人工智能是不可能的。报告称：“研究小组一致认为，在总体上规范人工智能的尝试可能是错误的。因为我们没有对人工智能的明确定义（很难一言以概之），而在不同的领域，其风险和考虑事项有着很大的不同。”</p>
<div class="com-insert-images"><figure style="margin:0;">&#13;
    <img alt="" data-ratio="0.591111" data-format="jpeg" class="lazyload" src="http://img.qdaily.com/uploads/20160903083810eIOFQdqusEv41NoJ.jpg-w600"/>&#13;
<figcaption class="">左起，亚马逊的杰夫·贝佐斯、IBM 的弗吉尼亚·罗曼提、微软的萨蒂亚·纳德拉、Google 的桑达尔·皮查伊和 Facebook 的马克·扎克伯格。图片版权：Eric Risberg / 美联社</figcaption></figure></div>
<p>斯通博士说，报告给出的一个建议是，提高各级政府对人工智能的警觉，丰富他们的专业知识。报告还呼吁增加公共及私人对人工智能的支出。</p>
<p>IBM Watson 人工智能部门经理大卫·肯尼（David Kenny）说：“政府在其中有它自己的作用，我们对此表示尊重……（问题是）很多时候，政策制定都落在了技术的后面。”</p>
<p>从五大巨头之间传阅的一份备忘录上可以看到，初步的设想是在九月中旬宣布成立新组织。据一位参与谈判的人士透露，一个尚未解决的问题是， Alphabet 旗下人工智能公司 Google DeepMind 要求以独立身份参加。</p>
<p>一位未被授权公开发言的人士从行业组织者口中得知，人工智能行业集团仿照的是类似人权组织全球网络倡议（<a href="https://www.globalnetworkinitiative.org/" rel="nofollow">Global Network Initiative</a>）的模式，该倡议组织中的各个企业和非政府组织主要关注于言论自由和隐私权的保护。</p>
<p>此外，LinkedIn 创始人、人工智能专业出身的雷德·霍夫曼（Reid Hoffman）与麻省理工学院媒体实验室（MIT Media Lab）也在进行相关讨论。他们计划资助一个项目，探索人工智能的社会和经济影响。</p>
<p>麻省理工的努力和行业合作都试图进一步密切技术进步和社会经济政策问题之间的联系。麻省理工的团队一直在讨论是否可以在新型人工智能和机器人系统的设计中纳入“社会因素”。</p>
<p>这就牵涉到了一个长期争论不休的问题：计算机和机器人系统是否还需要人类的介入。比如，美国国防部最近开始制定一项军事战略，需要用到人工智能，但生杀大权并没有委托给机器，而是仍掌握在人类手中。</p>
<p>麻省理工大学媒体实验室主任、《纽约时报》董事会成员伊藤穰一（Joichi Ito）说：“我要指出的关键问题是，计算机科学家一直不擅长与社会科学家和哲学家打交道。我们想要做的是支持那些正在做可以影响政策制定的研究的社会科学家，并向其提供援助。”</p>
<p>斯坦福报告试图明确一个问题，不久之后，典型的北美城市的公民将面对能模仿人类能力的计算机和机器人系统。报告作者探讨了现代生活的八个方面，包括医疗、教育、娱乐和就业，不过特意避开了战争。他们说，军事人工智能应用超出了他们现在的研究范围和专业知识，不过他们不排除未来会关注武器领域。</p>
<p>报告也没有考虑“奇点”的可能性。一些计算机科学家深信，一旦到达奇点，机器可能比人类更聪明，并且很可能对人类构成威胁。</p>
<p>斯通博士说：“该报告没有接受这点假设，这是个理性的决定。”</p>
<p>翻译 熊猫译社 陈晓斐<a name="_GoBack"/></p>
<p class="">题图来自 <a target="_blank" jsaction="mousedown:irc.rl;keydown:irc.rlk" data-noload="" tabindex="0" href="https://medium.com/arqueologias-do-futuro/eu-rob%C3%B4-f80efb701b42" data-ved="0ahUKEwjPkfHf-PHOAhVE2mMKHbY-B_kQjB0IBg" rel="nofollow">medium.com</a></p>
<p>© 2016 THE NEW YORK TIMES</p>

 <div class="embed-mask"><div class="embed-bd"><span class="triangle"/><div class="play"/></div><div class="embed-control"><span class="player"/><div class="bar"><span/></div><span class="sound iconfont icon-shengyin"/></div></div><p><img data-format="jpg" data-ratio="0.456667" class="lazyload lazylood" src="http://img.qdaily.com/uploads/20160725204735MvuXsg6iz3bhj7yi.jpg-w600" alt=""/></p><p class="lazylood">喜欢这篇文章？去 App 商店搜 <a href="http://m.qdaily.com/mobile/downloads/empty/2">好奇心日报</a> ，每天看点不一样的。</p></div></div>