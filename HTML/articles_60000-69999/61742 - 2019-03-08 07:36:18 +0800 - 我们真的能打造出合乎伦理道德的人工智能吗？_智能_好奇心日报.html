<h1>我们真的能打造出合乎伦理道德的人工智能吗？_智能_好奇心日报
</h1><p><img src="http://img.qdaily.com/article/article_show/20190305102446lRdhUCkv0beg7uQL.jpg?imageMogr2/auto-orient/thumbnail/!755x450r/gravity/Center/crop/755x450/ignore-error/1"></p><div class="article-detail-bd"><div class="author-share clearfix">   <div class="author">  <a rel="nofollow" href="javascript:void(0)" class="avatar x32 circle"><img src="http://m.qdaily.com/images/missing_face.png"/> </a>  <span class="name">Cade Metz</span><span class="date smart-date" data-origindate="2019-03-08 07:36:18 +0800"/><script language="JavaScript" type="text/javascript"><![CDATA[var o="div",a=" style='disp",b="lay:",c="none'";document.write("<"+o+a+b+c+">")]]></script><span class="date">2019-03-08 07:36:18</span><script language="JavaScript" type="text/javascript"><![CDATA[var o="div";document.write("</"+o+">")]]></script></div>      <div class="com-share-favor" data-id="61742" data-title="《我们真的能打造出合乎伦理道德的人工智能吗？》，来自@好奇心日报" data-pic="http://img.qdaily.com/article/article_show/20190305102446lRdhUCkv0beg7uQL.jpg?imageMogr2/auto-orient/thumbnail/!640x380r/gravity/Center/crop/640x380/ignore-error/1" data-url="http://www.qdaily.com/articles/61742.html" data-weiboappkey="2462590045"><div class="share-favor-bd clearfix"><a rel="nofollow" data-ga-event="pc:share:weixin" href="javascript:void(0)" class="share iconfont icon-weixin"/><a rel="nofollow" data-ga-event="pc:share:weibo" href="http://service.weibo.com/share/share.php" class="share iconfont icon-weibo"/><a rel="nofollow" data-ga-event="pc:share:tengxunweixin" href="http://share.v.t.qq.com/index.php" class="share iconfont icon-tengxunweibo"/><a rel="nofollow" data-ga-event="pc:share:kongjian" href="http://sns.qzone.qq.com/cgi-bin/qzshare/cgi_qzshare_onekey" class="share iconfont icon-kongjian"/><a rel="nofollow" data-ga-event="pc:share:douban" href="http://www.douban.com/share/service" class="share iconfont icon-douban"/><a rel="nofollow" data-ga-event="pc:share:linkedin" href="http://www.linkedin.com/shareArticle" class="share iconfont icon-linkedin"/>  <a rel="nofollow" data-ga-event="pc:favor:aritcle" href="#" class="favor iconfont icon-heart"><span class="num  smart-count" data-origincount="184"/></a>  </div></div> </div>  <p class="excerpt">“我们需要具备高尚的道德情操，才有资格来开发这项技术，同时我们还要制订一套明确的道德规范，让公众能够对我们进行监督。”</p>  <div class="detail">  

<p finallycleanhtml="true" nocleanhtml="true"><strong><em>＊<a href="https://www.nytimes.com/2019/03/01/business/ethics-artificial-intelligence.html" rel="nofollow">本文</a>只能在《好奇心日报》发布，即使我们允许了也不许转载＊</em></strong></p>
<p>加利福尼亚半月湾电 — 最近，一篇报道披露了人工智能（AI）公司 <a href="https://www.clarifai.com/" rel="nofollow">Clarifai</a> 与五角大楼达成合作协议的消息：他们正在开发一套 AI 图像识别系统，从而能够对无人机拍摄的视频片段进行分析。部分雇员认为开发这样的一套系统违背了社会伦理道德，但是该公司表示，项目一旦开发成功，将会最大程度地避免误伤情况的发生，从而起到拯救士兵及平民生命的作用。<br/></p>
<p>这家公司的创始人兼首席执行官、著名人工智能研究员马特·泽勒（Matt Zeiler）<a href="https://blog.clarifai.com/why-were-part-of-project-maven" rel="nofollow">在博客中写道</a>，Clarifai 的使命是通过不断改进 AI 技术来加速人类的发展进程。在<a href="https://www.fastcompany.com/90277470/despite-a-surge-of-tech-activism-clarifai-plans-to-push-further-into-government-work" rel="nofollow">接受新闻媒体采访</a>时，泽勒宣布公司将会增添一个新的管理职位，以确保公司所开发的所有项目都符合道德规范。<br/></p>
<p>越来越多的活动人士、研究人员和记者对 AI 的崛起表示担忧，他们发出了警告，称许多 AI 应用带有偏见性、欺骗性，甚至有些用途险恶。开发 AI 技术的公司对此做出回应，从 Google 和微软这样的科技巨头，到斗志昂扬的人工智能初创企业，许多公司都在制定企业原则，以确保他们的系统在开发以及使用的过程中都能做到合乎伦理道德。一些企业还设立了伦理顾问或审查委员会，确保公司能够按照设立的原则来开展工作。<br/></p>
<p>但随着越来越多的人质疑科技公司最终能否遵守许下的承诺，局势变得愈加紧张。考虑到企业可以改变路线，理想主义者可能会屈服于财政压力，一些活动人士、甚至是一些企业开始提出这样的观点：确保企业活动遵循伦理道德的唯一途径是政府监管。<br/></p>
<p>“我们不想看到各家公司为打破道德底线而竞争，因此有必要设立相应的法律条文。”微软总裁兼首席法律官布拉德·史密斯（Brad Smith）上周在《纽约时报》主办的加州半月湾“新工作峰会”（New Work Summit）上说。<br/></p>
<p><img data-format="jpg" data-ratio="0.361268" class="lazylaad lazyload" alt="" src="http://img.qdaily.com/uploads/20160918124612v5wnh60OmgCAUPGJ.jpg-w600"/></p>
<p>Clarifai 设立道德顾问职位一事，恐怕永远都不会成为现实。随着这家位于纽约市的初创企业进军军事领域，以及在面部识别系统的开发工作中取得新进展，一些员工越来越担心公司最终会开发出自动化战争系统或大规模监控系统。1 月底，忧心忡忡的员工在公司的留言板上发布了一封公开信，询问泽勒他们的工作最终将带领公司走向何方。<br/></p>
<p>三位员工因为担心遭到报复，匿名透露了以下信息：在发表公开信的数天后，泽勒召开了一次公司内部会议。他解释说，在内部设置道德顾问并不适用于像 Clarifai 这样的小公司。他还告诉员工，他们开发的技术最终将会应用到自动化武器系统上。<br/></p>
<p>Clarifai 公司专注于开发能够即时识别照片和视频中物体的技术。政策制定者称之为“<a href="https://www.nytimes.com/2019/01/01/technology/artificial-intelligence-export-restrictions.html?module=inline" rel="nofollow">两用技术</a>”。这类技术既有日常商业用途，比如在零售网站上识别名牌手袋，同时也有军事化用途，比如识别无人机目标。<br/></p>
<p>像 Clarifai 开发的这类 AI 技术，以及其他在近几年来发展迅速的 AI 系统可以起到改善<a href="https://www.nytimes.com/2018/01/04/technology/self-driving-cars-aurora.html?module=inline" rel="nofollow">交通</a>、优化<a href="https://www.nytimes.com/2019/02/11/health/artificial-intelligence-medical-diagnosis.html?module=inline" rel="nofollow">医疗体系</a>以及促进<a href="https://www.nytimes.com/2018/10/26/technology/earthquake-predictions-artificial-intelligence.html?module=inline" rel="nofollow">科学研究</a>的作用。不过，这些技术也使得<a href="https://www.nytimes.com/2018/07/08/business/china-surveillance-technology.html?module=inline" rel="nofollow">大规模监控</a>、<a href="https://www.nytimes.com/2018/02/20/technology/artificial-intelligence-risks.html?module=inline" rel="nofollow">网络钓鱼攻击</a>以及<a href="https://www.nytimes.com/2018/11/19/science/artificial-intelligence-deepfakes-fake-news.html?module=inline" rel="nofollow">虚假新闻的传播</a>变得更加便捷。</p>
<h3>发现偏见</h3>
<p>随着企业和政府将这些 AI 技术运用到实际生活中，研究人员也意识到，一些 AI 系统存在严重的偏见。例如，面部识别系统<a href="https://www.nytimes.com/2018/02/09/technology/facial-recognition-race-artificial-intelligence.html?module=inline" rel="nofollow">在识别女性或肤色较深的人时，准确率可能会明显下降</a>。其他系统可能会出现前所未见的安全漏洞。研究人员已经证明，可以利用无人驾驶汽车的设计漏洞，让系统认为空无一物的路面存在障碍物。<br/></p>
<p>所有这一切都意味着设计出合乎伦理道德的 AI 系统，是一项极其复杂的任务。当利益相关者意识到不同的人对伦理道德有不同的标准时，事情就变得更加困难了。<br/></p>
<p>在一些微软员工对公司所签订的军事合同提出抗议时，史密斯表示，美国科技公司一直以来都为军方提供帮助，他们也必须继续这样做。他在会议上说：“军方有责任保障美国的自由，我们必须支持那些冒着生命危险为自由而战的军人。”<br/></p>
<p>尽管 Clarifai 的一些员工明确地划定了道德界限，表示他们不会参与自动化武器的开发工作，但也有员工支持此类工作。泽勒认为，自动化武器最终将起到拯救生命的作用，因为与由人类操作者控制的武器相比它们更加精确。“AI 可以让武器变得更加精确，减少间接伤害事件的发生，从而降低平民伤亡率，它们还能避免发生友军误伤事件。”<br/></p>
<p>Google 曾经与 Clarifai 一道参与了五角大楼的项目开发工作，但在公司员工的抗议声中，这家科技巨头最终退出了这个项目。不过，还有<a href="https://www.nytimes.com/2018/08/26/technology/pentagon-artificial-intelligence.html?module=inline" rel="nofollow">其他 20 多家科技公司</a>像 Clarifai 一样，并没有屈服于来自伦理方面的压力，继续参与项目开发工作。<br/></p>
<p>在五角大楼项目引发争议后，Google 制定了一套“<a href="https://www.blog.google/technology/ai/ai-principles/" rel="nofollow">AI 开发原则</a>”，旨在给未来的项目提供道德指导意见。但即便公司制订了开发准则，一些员工还是以离职的方式来表示抗议。一方面，这些新制订的开发原则含糊不清，可作各种诠释；另一方面，负责确保公司在开发过程中遵守这些道德准则的高管，同时也负有保护公司财务利益不受损害的责任。<br/></p>
<p>去年年底离职的 Google 前员工利兹·方-琼斯（Liz Fong-Jones）说：“从实用性角度讲，这些准则是典型的狐狸看守鸡舍的情况。”<br/></p>
<p>2014 年，当 Google 收购了全球最重要的人工智能实验室 DeepMind 时，这家公司同意成立一个外部审查委员会，以确保研究成果不会被用于开发军事武器，或是其他不符合伦理道德的项目。如今五年过去了，我们仍不清楚这个委员会是否存在。<br/></p>
<p>Google、微软、Facebook 和其他科技公司已经创建了像 AI 合作组织（Partnership on A.I.）这类机构，旨在确保各家公司在开发项目时遵守行业伦理准则。但这些组织并没有什么实际的约束力。<br/></p>
<div class="com-insert-images"><figure style="margin: 0px;"> <img data-ratio="0.666992" data-format="jpeg" class="lazyload" src="http://img.qdaily.com/uploads/20190305102529A2yZLkWRIcJofds6.jpg-w600"/><figcaption>图片版权：John Hersey</figcaption> </figure></div>
<h3>员工开始发声</h3>
<p>在 AI 行业，公司员工发起的抗议活动推动了许多重大的行业变革，比如 Google 的职工抗议事件，而专家和其他独立学者的有针对性的研究也为改变行业现状贡献了一份力量。去年，亚马逊的一些员工对公司将面部识别服务出售给警察部门一事提出了抗议，各项学术研究也纷纷指出人脸识别系统存在偏见，这些事件发生后，亚马逊和微软都希望政府能够在这一领域实行监管。<br/></p>
<p>“人们已经看出了 AI 系统存在一些问题，意识到需要作出改变。”梅雷迪思·惠特克（Meredith Whittaker）说道。她不仅是一名 Google 员工，还是 <a href="https://ainowinstitute.org/" rel="nofollow">AI Now 机构</a>的联合创始人，这个研究组织负责评估人工智能可能会为社会带来的影响。她在峰会上表示，随着资本主义的力量继续迫使这些科技公司从市场中获取更大的利润，实现变化需要一个漫长的过程。<br/></p>
<p>Clarifai 的员工担心，为开发面部识别系统所编写的 AI 程序最终将为自动化武器系统的出现铺平道路，而这些程序本身就存在许多缺陷，这无异于打开了潘多拉魔盒。致泽勒的公开信中写道：“技术可能会被不法分子利用，做出一些背离设计初衷的事情；黑客可以劫持 AI 系统为所欲为；这些系统还不可避免地存在偏见。我们业内人士明白其中的利害。”<br/></p>
<p>数千名各行各业的<a href="https://www.theguardian.com/science/2018/jul/18/thousands-of-scientists-pledge-not-to-help-build-killer-ai-robots" rel="nofollow">人工智能研究员</a>签署了另一封公开信，称他们不支持自动化武器的开发工作。<br/></p>
<p>五角大楼表示，Google 和 Clarifai 等公司所开发的 AI 系统并没有被用于研发进攻性武器。近年来，全球大多数顶尖的人工智能研究员都集中在工业界，军方为了得到他们的支持，目前正在建立起一套相关的原则体系。<br/></p>
<p>但许多政策专家表示，军方制订的相关原则不太可能会比大公司所制定的原则具有更大的影响力，因为五角大楼开发类似技术的原因是为了不在 AI 武器领域落后于中国、俄罗斯和其他国际竞争对手。出于这个原因，一些人呼吁签订国际条约，禁止各国使用自动化武器。<br/></p>
<h3>监管足以解决问题吗？</h3>
<p>Clarifai 的员工在<a href="https://int.nyt.com/data/documenthelper/639-clarifai-letter/3cd943d873d78c7cdcdc/optimized/full.pdf#page=1" rel="nofollow">公开信</a>中说，他们不确定监管将成为解决 AI 伦理道德问题的答案，他们认为公司本身负有直接责任。<br/></p>
<p>“监管将减缓进步的速度，而美国需要不断进步，才能在今天所面临的诸多威胁中生存下来。”他们在写给泽勒和公司其他成员的信中写道。“我们需要具备高尚的道德情操，才有资格来开发这项技术，同时我们还要制订一套明确的道德规范，让公众能够对我们进行监督。”<br/></p>
<p>然而，公开信并没有达到预期的效果。就在泽勒于内部会议中公布了 Clarifai 极有很可能会开展自动化武器研发工作的几天后，公开信的作者利兹奥·沙利文（Liz O’sullivan）离开了这家公司，他本来是伦理顾问的人选。<br/></p>
<p>许多像惠特克一样的研究员兼活动人士认为，目前这个时代对于科技公司员工来说非常有利，因为他们可以利用自己的力量来推动变革。但他们也表示，除了科技公司外，社会各界都必须重视 AI 所带来的伦理问题。<br/></p>
<p>惠特克说道：“我们需要政府监管。就连微软的首席法律官布拉德·史密斯也表示，想要让 AI 技术能够更好地造福人类，我们需要设立一套监管体系。”<br/></p>
<p><br/></p>
<p>翻译：熊猫译社 驰逸</p>
<p>题图版权：John Hersey</p>
<p>© 2019 THE NEW YORK TIMES</p>

 <div class="embed-mask"><div class="embed-bd"><span class="triangle"/><div class="play"/></div><div class="embed-control"><span class="player"/><div class="bar"><span/></div><span class="sound iconfont icon-shengyin"/></div></div><p><img data-format="jpg" data-ratio="0.456667" class="lazyload lazylood" src="http://img.qdaily.com/uploads/20160725204735MvuXsg6iz3bhj7yi.jpg-w600" alt=""/></p><p class="lazylood">喜欢这篇文章？去 App 商店搜 <a href="http://m.qdaily.com/mobile/downloads/empty/2">好奇心日报</a> ，每天看点不一样的。</p></div></div>