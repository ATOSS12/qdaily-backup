<h1>机器人也会产生偏见，IBM 打算解决它_智能_好奇心日报
</h1><p><img src="http://img.qdaily.com/article/article_show/201809201429597iadzefscrPqByt9.jpg?imageMogr2/auto-orient/thumbnail/!755x450r/gravity/Center/crop/755x450/ignore-error/1"></p><div class="article-detail-bd"><div class="author-share clearfix">   <div class="author">  <a rel="nofollow" href="javascript:void(0)" class="avatar x32 circle"><img src="http://img.qdaily.com/user/face/20180403082211d5EYRce6aS2plAyz.jpg?imageMogr2/auto-orient/thumbnail/!80x80r/gravity/Center/crop/80x80/ignore-error/1"/> </a>  <span class="name">王毓婵</span><span class="date smart-date" data-origindate="2018-09-21 06:32:00 +0800"/><script language="JavaScript" type="text/javascript"><![CDATA[var o="div",a=" style='disp",b="lay:",c="none'";document.write("<"+o+a+b+c+">")]]></script><span class="date">2018-09-21 06:32:00</span><script language="JavaScript" type="text/javascript"><![CDATA[var o="div";document.write("</"+o+">")]]></script></div>      <div class="com-share-favor" data-id="56597" data-title="《机器人也会产生偏见，IBM 打算解决它》，来自@好奇心日报" data-pic="http://img.qdaily.com/article/article_show/201809201429597iadzefscrPqByt9.jpg?imageMogr2/auto-orient/thumbnail/!640x380r/gravity/Center/crop/640x380/ignore-error/1" data-url="http://www.qdaily.com/articles/56597.html" data-weiboappkey="2462590045"><div class="share-favor-bd clearfix"><a rel="nofollow" data-ga-event="pc:share:weixin" href="javascript:void(0)" class="share iconfont icon-weixin"/><a rel="nofollow" data-ga-event="pc:share:weibo" href="http://service.weibo.com/share/share.php" class="share iconfont icon-weibo"/><a rel="nofollow" data-ga-event="pc:share:tengxunweixin" href="http://share.v.t.qq.com/index.php" class="share iconfont icon-tengxunweibo"/><a rel="nofollow" data-ga-event="pc:share:kongjian" href="http://sns.qzone.qq.com/cgi-bin/qzshare/cgi_qzshare_onekey" class="share iconfont icon-kongjian"/><a rel="nofollow" data-ga-event="pc:share:douban" href="http://www.douban.com/share/service" class="share iconfont icon-douban"/><a rel="nofollow" data-ga-event="pc:share:linkedin" href="http://www.linkedin.com/shareArticle" class="share iconfont icon-linkedin"/>  <a rel="nofollow" data-ga-event="pc:favor:aritcle" href="#" class="favor iconfont icon-heart"><span class="num  smart-count" data-origincount="68"/></a>  </div></div> </div>  <p class="excerpt">机器人的偏见是如何产生的？</p>  <div class="detail">  

<p finallycleanhtml="true" nocleanhtml="true"><a href="https://techcrunch.com/2018/09/19/ibm-launches-cloud-tool-to-detect-ai-bias-and-explain-automated-decisions/" rel="nofollow">据 TechCrunch 消息</a>，IBM 推出了一项新的云服务，可以扫描人工智能系统的工作情况，检测系统偏见，并为正在做出的自动决策提供解释。</p>
<p>所谓的“系统偏见”指的是人工智能系统在运算过程中产生的误差——它可能是由原始数据积累地不全面导致的，也可能是由计算设定中的 bug 导致的，或者是由工程师错误的操作方式导致的。</p>
<p>一个比较典型的例子是，亚马逊的开放式人脸识别系统 Rekognition 将 28 个美国国会议员认成了嫌疑犯。</p>
<p>这其中的原因，一是因为测试者使用的默认置信度阈值是 80%，而亚马逊建议执法部门应用该程序时应至少设置 95% 的置信度门槛；二是因为如果人工智能的训练数据本身就是严重偏向白人的，那么机器做出的判断可能也会非常偏向白人。整个国会中只有 20% 的有色人种，而在机器认定的“嫌疑犯”中，有色人种却占了 39%。更早前 <a href="https://www.nist.gov/programs-projects/face-recognition-vendor-test-frvt-ongoing" rel="nofollow">NIST 的面部识别供应商测试</a>也发现，女性和非洲裔美国人被 Rekognition 误识的概率更高。</p>
<p>类似的例子还有，亚马逊和 Google 的语音助手在理解非美国口音信息时，准确率会比理解美国口音信息下降 30%。</p>
<p><img data-format="jpg" data-ratio="0.372796" class="lazylaad lazyload" alt="" src="http://img.qdaily.com/uploads/20160918124623tvJN5Uq9dWRZ60jS.jpg-w600"/></p>
<p>IBM 新推出的云服务致力于解决这一问题。它可以检测运行中的 AI 系统中存在的偏见，在不公平的结果产生时进行“捕获”，也可以自动提出修改建议。</p>
<p>除了消除系统误差，IBM 还可以对判断做出解释，比如哪些因素加重了哪个方向的判断，我们应该对判断结果抱有多大的信心，以及是哪些因素决定了我们抱有这个程度的信心。</p>
<p>IBM 称，这一系统运行在 IBM 云上，与 IBM 的机器学习框架和 AI 模型兼容，包括其 Watson 技术，以及 Tensorflow、SparkML、AWS SageMaker 和 AzureML 等。</p>
<p>欧盟的 GDPR<a href="https://techcrunch.com/2018/01/20/wtf-is-gdpr/" rel="nofollow"> 隐私法案规定</a>，普通用户有权了解人工智能算法在特定情况下如何工作。这意味着许多依赖人工智能系统的科技公司可能需要一套能够审核他们的 AI ，并为其判断做出解释的工具。</p>
<p>IBM 不是第一家发现 AI 偏见商业机会的专业服务公司。几个月前，<a href="https://techcrunch.com/2018/06/09/accenture-wants-to-beat-unfair-ai-with-a-professional-toolkit/" rel="nofollow">埃森哲</a>也公开了一个工具，用于识别和修复不公平的 AI。“没有一个算法是完美的。”埃森哲人工智能业务负责人 Rumman Chowdhury 说，“我们知道模型有时会出错。但对于不同的人存在不同程度的错误是不公平的。”<br/></p>
<p>埃森哲的工具可以检查 AI 系统中的敏感变量，如年龄、性别、种族等，从而判断其运算结果是否有倾斜倾向。<br/></p>
<p>IBM 发言人称，识别和修复 AI 偏见的工具的出现，会鼓励更多企业有信心使用人工智能系统。根据 IBM 的研究，虽然有 82％ 的企业正在考虑使用人工智能技术，但有 60％ 担心后续带来的责任问题，有 63％ 的企业担心缺乏人才来管理人工智能技术。<br/></p>
<p>不过提到偏见，IBM 自己也正因一款天生带有“偏见”的软件而<a href="http://www.qdaily.com/articles/56181.html">陷入风波</a>。美国科技媒体 The Intercept <a href="https://theintercept.com/2018/09/06/nypd-surveillance-camera-skin-tone-search/" rel="nofollow">本月报道称</a>，纽约警方的视频监控系统软件由 IBM 公司开发，纽约市警察局则提供大量的闭路电视监控数据作为软件训练的数据库，最终让软件可以根据肤色、发色、性别、年龄和各种面部特征去搜索相关人员。</p>
<p>通过皮肤、种族等信息进行嫌疑人筛选的监控系统引起了很大的争议，公民自由倡导者组织表示，他们对纽约警察局帮助建立具有潜在大规模种族貌区别的监控系统的计划感到震惊。<br/></p>
<p>IBM 开发的去除偏见工具也许对他们自己的软件也有用。</p>
<p><a href="https://visualhunt.com/f2/photo/540616805/a77a570c49/" rel="nofollow">题图/visualhunt</a><br/></p>

 <div class="embed-mask"><div class="embed-bd"><span class="triangle"/><div class="play"/></div><div class="embed-control"><span class="player"/><div class="bar"><span/></div><span class="sound iconfont icon-shengyin"/></div></div><p><img data-format="jpg" data-ratio="0.456667" class="lazyload lazylood" src="http://img.qdaily.com/uploads/20160725204735MvuXsg6iz3bhj7yi.jpg-w600" alt=""/></p><p class="lazylood">喜欢这篇文章？去 App 商店搜 <a href="http://m.qdaily.com/mobile/downloads/empty/2">好奇心日报</a> ，每天看点不一样的。</p></div></div>