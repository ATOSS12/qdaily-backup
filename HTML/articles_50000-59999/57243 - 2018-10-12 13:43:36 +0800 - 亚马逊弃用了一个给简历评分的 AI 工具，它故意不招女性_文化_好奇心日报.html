<h1>亚马逊弃用了一个给简历评分的 AI 工具，它故意不招女性_文化_好奇心日报
</h1><p><img src="http://img.qdaily.com/article/article_show/201810121118089WmbCRB0VY1Kd6Hc.jpeg?imageMogr2/auto-orient/thumbnail/!755x450r/gravity/Center/crop/755x450/ignore-error/1"></p><div class="article-detail-bd"><div class="author-share clearfix">   <div class="author">  <a rel="nofollow" href="javascript:void(0)" class="avatar x32 circle"><img src="http://img.qdaily.com/user/face/20190515025459GWdxlOAQSgqX1sMo.png?imageMogr2/auto-orient/thumbnail/!80x80r/gravity/Center/crop/80x80/ignore-error/1"/> </a>  <span class="name">顾天鹂</span><span class="date smart-date" data-origindate="2018-10-12 13:43:36 +0800"/><script language="JavaScript" type="text/javascript"><![CDATA[var o="div",a=" style='disp",b="lay:",c="none'";document.write("<"+o+a+b+c+">")]]></script><span class="date">2018-10-12 13:43:36</span><script language="JavaScript" type="text/javascript"><![CDATA[var o="div";document.write("</"+o+">")]]></script></div>      <div class="com-share-favor" data-id="57243" data-title="《亚马逊弃用了一个给简历评分的 AI 工具，它故意不招女性》，来自@好奇心日报" data-pic="http://img.qdaily.com/article/article_show/201810121118089WmbCRB0VY1Kd6Hc.jpeg?imageMogr2/auto-orient/thumbnail/!640x380r/gravity/Center/crop/640x380/ignore-error/1" data-url="http://www.qdaily.com/articles/57243.html" data-weiboappkey="2462590045"><div class="share-favor-bd clearfix"><a rel="nofollow" data-ga-event="pc:share:weixin" href="javascript:void(0)" class="share iconfont icon-weixin"/><a rel="nofollow" data-ga-event="pc:share:weibo" href="http://service.weibo.com/share/share.php" class="share iconfont icon-weibo"/><a rel="nofollow" data-ga-event="pc:share:tengxunweixin" href="http://share.v.t.qq.com/index.php" class="share iconfont icon-tengxunweibo"/><a rel="nofollow" data-ga-event="pc:share:kongjian" href="http://sns.qzone.qq.com/cgi-bin/qzshare/cgi_qzshare_onekey" class="share iconfont icon-kongjian"/><a rel="nofollow" data-ga-event="pc:share:douban" href="http://www.douban.com/share/service" class="share iconfont icon-douban"/><a rel="nofollow" data-ga-event="pc:share:linkedin" href="http://www.linkedin.com/shareArticle" class="share iconfont icon-linkedin"/>  <a rel="nofollow" data-ga-event="pc:favor:aritcle" href="#" class="favor iconfont icon-heart"><span class="num  smart-count" data-origincount="97"/></a>  </div></div> </div>  <p class="excerpt">跟人类学的啊</p>  <div class="detail">  

<p finallycleanhtml="true" nocleanhtml="true"><a href="https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G" rel="nofollow">路透社近日披露</a>，亚马逊在去年弃用了一个筛简历的 AI 工具——它好像发展出了性别歧视倾向，直接通过关键词筛掉了一堆女应聘者。</p>
<p>信源告诉路透社，亚马逊从 2014 年起就开始开发这个用于评价简历的程序，它会给所有简历按 1-5 星打分（就像在亚马逊上给货品评分一样）。</p>
<p>“每个人都想要圣杯，” 一个信源说道，“他们真的就想把这个软件发展成那样的引擎，给它 100 个简历，它立马给你排出最优秀的 5 个，我们直接雇佣他们就好了。”</p>
<p>可是，这个程序被人类投喂的训练资料是过去 10 年亚马逊收到的简历。它在回顾这些简历后挑选出 5 万个关键词用作评判标准，学习到的是人类 HR 招人的模式。</p>
<p>该模式显然有不完善之处。在这个本来就由男性主导的科技界，招聘中有话语权的人因为种种原因更乐意招入男性。这也告诉了系统，具有男性特质的简历总会是公司青睐的选择，因而它耿直地把带有“女子的”（women’s）名词降权（比如“女子象棋俱乐部”），直接筛掉两所女校的毕业生，并且倾向使用“男性化”用语的简历，比如“执行”（executed）和“捕捉”（captured）。</p>
<p>工程师们在一年后发现了这个问题，立刻修改了程序，期望它在某些问题的判断上对所有人都持中立”态度。然而这并不能保证软件不会在其他地方、以别的形式歧视某个人群。而且亚马逊还发现，软件有时会留下根本不达标的候选人，智能比较有限。他们最终在去年年初放弃了这个工具，因为高层对它的表现失去了信心。</p>
<p>有越来越多的公司开始使用 AI 软件筛第一轮简历，据悉有 55% 的美国人力资源经理说 AI 会在 5 年内成为他们日常工作的助手。这个案例可能会给予他们一些启示——机器从人类给予的材料中习得他们潜意识中的偏见，最终在“算法偏见”的影响下区别对待被评判的对象，做出错误判断或显示错误结果。</p>
<p><img data-format="jpg" data-ratio="0.426075" class="lazyloadd lazyload" alt="" src="http://img.qdaily.com/uploads/20160918124606JSpdaCk3eAlgqD5x.jpg-w600"/></p>
<p>这种机器偏见跨越各个领域：一个叫做 COMPAS 的系统通过计算罪犯再次犯罪的可能来给罪犯量刑提供指导，<a href="https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing" rel="nofollow">但是它的结果明显表现了种族倾向</a>，当一个白人犯罪老手和黑人新手同天被录入系统时，机器认为黑人更可能在未来犯罪（两年后，白人因抢劫罪被判刑 8 年，黑人未受任何指控）；今年 2 月，<a href="https://www.newscientist.com/article/2166207-discriminating-algorithms-5-times-ai-showed-prejudice/" rel="nofollow">MIT 发现三大性别识别 AI（分别来自 IBM、微软和旷视）判断白人男性的准确率达到 99%，对黑人女性的准确率是 35%</a>，因为在它们接受的识图训练里，白人男性数量远远超过黑人女性。</p>
<p><a href="https://www.economist.com/special-report/2016/06/25/frankensteins-paperclips" rel="nofollow">深度学习公司 MetaMind Richard Socher 曾指出</a>，AI 已经渗透了我们的生活，所以它们不带任何偏见是至关重要的，没人有意将系统设计得具有偏见，但是“如果用糟糕的数据去训练它，它就会做出糟糕的判断”。</p>
<p><a href="https://www.technologyreview.com/s/608248/biased-algorithms-are-everywhere-and-no-one-seems-to-care/" rel="nofollow">令专家们感到担忧的是</a>，虽然有越来越多的系统显出了偏见，但是很多公司和政府决策者，要么过于相信系统，要么对限制这种偏见并不感兴趣。</p>
<p>题图来自 <a href="https://www.pexels.com/photo/adult-agreement-blur-brainstorming-630839/" rel="nofollow">Pexels</a></p>

 <div class="embed-mask"><div class="embed-bd"><span class="triangle"/><div class="play"/></div><div class="embed-control"><span class="player"/><div class="bar"><span/></div><span class="sound iconfont icon-shengyin"/></div></div><p><img data-format="jpg" data-ratio="0.456667" class="lazyload lazylood" src="http://img.qdaily.com/uploads/20160725204735MvuXsg6iz3bhj7yi.jpg-w600" alt=""/></p><p class="lazylood">喜欢这篇文章？去 App 商店搜 <a href="http://m.qdaily.com/mobile/downloads/empty/2">好奇心日报</a> ，每天看点不一样的。</p></div></div>