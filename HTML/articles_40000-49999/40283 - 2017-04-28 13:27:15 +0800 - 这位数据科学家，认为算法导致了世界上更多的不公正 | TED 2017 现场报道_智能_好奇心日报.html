<h1>这位数据科学家，认为算法导致了世界上更多的不公正 | TED 2017 现场报道_智能_好奇心日报
</h1><p><img src="http://img.qdaily.com/article/article_show/20170428093125mj62MkcLzRxqs9gr.png?imageMogr2/auto-orient/thumbnail/!755x450r/gravity/Center/crop/755x450/ignore-error/1"></p><div class="article-detail-bd"><div class="author-share clearfix">   <div class="author">  <a rel="nofollow" href="javascript:void(0)" class="avatar x32 circle"><img src="http://img.qdaily.com/user/face/20160702194404LNAgRmHwrTzpEquZ.jpg?imageMogr2/auto-orient/thumbnail/!80x80r/gravity/Center/crop/80x80/ignore-error/1"/> </a>  <span class="name">谢若含</span><span class="date smart-date" data-origindate="2017-04-28 13:27:15 +0800"/><script language="JavaScript" type="text/javascript"><![CDATA[var o="div",a=" style='disp",b="lay:",c="none'";document.write("<"+o+a+b+c+">")]]></script><span class="date">2017-04-28 13:27:15</span><script language="JavaScript" type="text/javascript"><![CDATA[var o="div";document.write("</"+o+">")]]></script></div>      <div class="com-share-favor" data-id="40283" data-title="《这位数据科学家，认为算法导致了世界上更多的不公正 | TED 2017 现场报道》，来自@好奇心日报" data-pic="http://img.qdaily.com/article/article_show/20170428093125mj62MkcLzRxqs9gr.png?imageMogr2/auto-orient/thumbnail/!640x380r/gravity/Center/crop/640x380/ignore-error/1" data-url="http://www.qdaily.com/articles/40283.html" data-weiboappkey="2462590045"><div class="share-favor-bd clearfix"><a rel="nofollow" data-ga-event="pc:share:weixin" href="javascript:void(0)" class="share iconfont icon-weixin"/><a rel="nofollow" data-ga-event="pc:share:weibo" href="http://service.weibo.com/share/share.php" class="share iconfont icon-weibo"/><a rel="nofollow" data-ga-event="pc:share:tengxunweixin" href="http://share.v.t.qq.com/index.php" class="share iconfont icon-tengxunweibo"/><a rel="nofollow" data-ga-event="pc:share:kongjian" href="http://sns.qzone.qq.com/cgi-bin/qzshare/cgi_qzshare_onekey" class="share iconfont icon-kongjian"/><a rel="nofollow" data-ga-event="pc:share:douban" href="http://www.douban.com/share/service" class="share iconfont icon-douban"/><a rel="nofollow" data-ga-event="pc:share:linkedin" href="http://www.linkedin.com/shareArticle" class="share iconfont icon-linkedin"/>  <a rel="nofollow" data-ga-event="pc:favor:aritcle" href="#" class="favor iconfont icon-heart"><span class="num  smart-count" data-origincount="172"/></a>  </div></div> </div>  <p class="excerpt">“算法不过就是让现实里存在的偏见变得自动化了”</p>  <div class="detail">  

<p finallycleanhtml="true" nocleanhtml="true">“ 我这边马上就要上 TED 演讲了，希望等会不要晕倒在舞台上，另一边他们还在卖我的书《数学毁灭性武器》”数据科学家 Cathy O'Neil 在登上今年的 TED 舞台以前，<a href="https://twitter.com/mathbabedotorg" rel="nofollow">在 Twitter 上</a>这样打趣自己。</p>
<p>但她这次的演讲主题很严肃，Cathy O'Neil 想告诉人们那些看似有大量数据做支撑的算法得出的结果，其实并不客观。相反它背后是由人们的偏见所掌控的。但算法如今在我们生活中实在是太常见了，从你缴纳医保费用的多少到申请一份工作，背后可能都是算法在进行衡量。</p>
<p>Cathy O'Neil 拿到哈佛大学数学系博士学位后，又在华尔街冲基金公司担任量化策略分析师多年，了解金融机构内部运作机制之后，Cathy O'Neil 对基金模型大失所望。发生全球金融危机之后，她成为了“占领华尔街运动”的主要发起人之一，以此来反对大公司的贪婪与不公正。</p>
<p>在演讲中，Cathy O'Neil 通过大量事例证明了由算法来得出的判断可能是错误的。她引述了<a href="http://www.nytimes.com/2011/03/07/education/07winerip.html" rel="nofollow">《纽约时报》曾报道</a>过一位教师的例子，在纽约考核教师的算法机制中，除了学生的学业表现，还要综合考虑上 20-30 种变量。最后这个算法对教师考评的结果让这所学校最优秀的老师，得到了最差的评价等级。不仅是在绩效考核领域，来自<a href="https://www.propublica.org/series/machine-bias" rel="nofollow">媒体调查机构 Propublica</a> 的一份数据也显示，算法歧视体现在各个领域，包括预判犯罪、评估汽车保险费用等。</p>
<div class="com-insert-images"><figure style="margin: 0px;"> <img data-ratio="0.588983" data-format="png" class="lazyload" src="http://img.qdaily.com/uploads/20170428123803N39CjaPFcMxBzJW7.png-w600"/><figcaption>通过预测模型，左边的犯罪风险为高，右边为低。但事实并非如此</figcaption> </figure></div>
<p>Cathy O'Neil 还假设，如果 Fox 新闻决定通过算法来改变过去性虐待丑闻的负面形象，他们在制定这套算法时，首先需要数据来进行大量的训练。数据从哪儿来？ 从过往这家公司 20 年来的招聘数据库里来。</p>
<p><img data-format="jpg" data-ratio="0.433463" class="lazylod lazyload" alt="" src="http://img.qdaily.com/uploads/20160918124618WGE68r1QqaUwOxkt.jpg-w600"/></p>
<p>Cathy O'Neil 认为，这套算法依然会对女性产生歧视，因为这些用来训练算法的数据在以往的招聘中就一直存在偏见，它所得出的算法就不可能客观。</p>
<p>“算法并没有让事情变得更加公平。” O'Neil 说。</p>
<p><a href="https://www.nytimes.com/2016/10/16/books/review/the-story-behind-this-weeks-best-sellers.html?_r=0&amp;module=ArrowsNav&amp;contentCollection=Book%20Review&amp;action=keypress&amp;region=FixedLeft&amp;pgtype=article" rel="nofollow">在她的《数学毁灭性武器》</a>（《<em>Weapons of Math Destruction</em>》）一书中也详细论述了大数据与机器模型的负面影响。总体来讲，O'Neil认为，这些模型对在社会中占劣势的人有负面的影响，同时让富裕的人过得更轻松；同时，得出这些模型的过程并不是透明的，这让人们难以理解并发现其中的问题。</p>
<p> O'Neil 还告诉《好奇心日报》，在离开金融公司，转型为一名社会活动家之后，将关注点放到了影响公众的算法上。比如 Google 的图片识别技术为什么会把黑人识别成猩猩呢，这就是因为在这个算法的数据训练库当中，根本没有放入过黑人的相册进行测试。”</p>
<p>要如何解决这个问题？ </p>
<p>O'Neil 觉得不能指望市场来解决 “自由市场无法解决任何问题，因为人们对利益的追逐驱使算法变得不公正。”</p>
<p>她的建议是，让算法数据也能得到审计，审计内容包括检测数据公正性（分析用于构建算法的数据是否含有偏见）、界定对于结果成功的定义（确保算法支持的是一个不带任何偏见色彩的目标）、检查整体的准确度、控制算法长期影响，避免数据的反馈回路。</p>
<p>最后，O'Neil 认为要靠政府监管者的力量确保这一切能够得到实施。</p>
<p>但她又补了一句 “我并不指望川普政府。” </p>
<p><br/></p>
<p>题图来自 TED</p>

 <div class="embed-mask"><div class="embed-bd"><span class="triangle"/><div class="play"/></div><div class="embed-control"><span class="player"/><div class="bar"><span/></div><span class="sound iconfont icon-shengyin"/></div></div><p><img data-format="jpg" data-ratio="0.456667" class="lazyload lazylood" src="http://img.qdaily.com/uploads/20160725204735MvuXsg6iz3bhj7yi.jpg-w600" alt=""/></p><p class="lazylood">喜欢这篇文章？去 App 商店搜 <a href="http://m.qdaily.com/mobile/downloads/empty/2">好奇心日报</a> ，每天看点不一样的。</p></div></div>