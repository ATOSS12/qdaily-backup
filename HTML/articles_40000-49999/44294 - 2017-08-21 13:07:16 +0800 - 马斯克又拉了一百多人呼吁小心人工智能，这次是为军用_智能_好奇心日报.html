<h1>马斯克又拉了一百多人呼吁小心人工智能，这次是为军用_智能_好奇心日报
</h1><p><img src="http://img.qdaily.com/article/article_show/20170821105218JxK4OFYzhvusqTkG.jpg?imageMogr2/auto-orient/thumbnail/!755x450r/gravity/Center/crop/755x450/ignore-error/1"></p><div class="article-detail-bd"><div class="author-share clearfix">   <div class="author">  <a rel="nofollow" href="javascript:void(0)" class="avatar x32 circle"><img src="http://img.qdaily.com/user/face/20160703130236ZQpY0h4qv7aEeUzs.jpg?imageMogr2/auto-orient/thumbnail/!80x80r/gravity/Center/crop/80x80/ignore-error/1"/> </a>  <span class="name">徐弢</span><span class="date smart-date" data-origindate="2017-08-21 13:07:16 +0800"/><script language="JavaScript" type="text/javascript"><![CDATA[var o="div",a=" style='disp",b="lay:",c="none'";document.write("<"+o+a+b+c+">")]]></script><span class="date">2017-08-21 13:07:16</span><script language="JavaScript" type="text/javascript"><![CDATA[var o="div";document.write("</"+o+">")]]></script></div>      <div class="com-share-favor" data-id="44294" data-title="《马斯克又拉了一百多人呼吁小心人工智能，这次是为军用》，来自@好奇心日报" data-pic="http://img.qdaily.com/article/article_show/20170821105218JxK4OFYzhvusqTkG.jpg?imageMogr2/auto-orient/thumbnail/!640x380r/gravity/Center/crop/640x380/ignore-error/1" data-url="http://www.qdaily.com/articles/44294.html" data-weiboappkey="2462590045"><div class="share-favor-bd clearfix"><a rel="nofollow" data-ga-event="pc:share:weixin" href="javascript:void(0)" class="share iconfont icon-weixin"/><a rel="nofollow" data-ga-event="pc:share:weibo" href="http://service.weibo.com/share/share.php" class="share iconfont icon-weibo"/><a rel="nofollow" data-ga-event="pc:share:tengxunweixin" href="http://share.v.t.qq.com/index.php" class="share iconfont icon-tengxunweibo"/><a rel="nofollow" data-ga-event="pc:share:kongjian" href="http://sns.qzone.qq.com/cgi-bin/qzshare/cgi_qzshare_onekey" class="share iconfont icon-kongjian"/><a rel="nofollow" data-ga-event="pc:share:douban" href="http://www.douban.com/share/service" class="share iconfont icon-douban"/><a rel="nofollow" data-ga-event="pc:share:linkedin" href="http://www.linkedin.com/shareArticle" class="share iconfont icon-linkedin"/>  <a rel="nofollow" data-ga-event="pc:favor:aritcle" href="#" class="favor iconfont icon-heart"><span class="num  smart-count" data-origincount="31"/></a>  </div></div> </div>  <p class="excerpt">这种武器还没有被开发，但联合国、技术公司都有担忧。</p>  <div class="detail">  

<p finallycleanhtml="true">马斯克又出来表达对人工智能的担忧，这次还有其它 115 位科技公司的创始人。</p>
<p><a href="https://www.theguardian.com/technology/2017/aug/20/elon-musk-killer-robots-experts-outright-ban-lethal-autonomous-weapons-war" rel="nofollow">《卫报》周日报道称</a>，来自 26 个国家、116 位机器人和人工智能公司的创始人递交了一封<a href="https://futureoflife.org/autonomous-weapons-open-letter-2017" rel="nofollow">给联合国的建议信</a>，希望后者可以禁止所谓“致命性自动化武器（LAWS）”的部署。其中包括特斯拉创始人马斯克，DeepMind 联合创始人 Mustafa Suleyman。</p>
<p>这里的“致命性自动化武器（LAWS）”不是指具体某一种武器。按照<a href="https://en.wikipedia.org/wiki/Lethal_autonomous_weapon" rel="nofollow">其定义</a>，是机器人在没有人为操作下，自动筛选和攻击目标。</p>
<p>相比之下，目前军用无人机很多虽然可以搭载导弹杀人，但它攻击什么目标，是否攻击都由电脑前的人类操控。</p>
<p>在更通俗的说法中，这种武器还会被称作是杀人机器（killer robot）。</p>
<p>把这封信刊登出来的，是一家被名为“未来人生协会（Future of Life Institute）”的网站。</p>
<p><img data-format="jpg" data-ratio="0.390852" class="lazyloadd lazyload" alt="" src="http://img.qdaily.com/uploads/20160918124612v5wnh60OmgCAUPGJ.jpg-w600"/></p>
<p>这封信表达的是这些科技领袖对联合国发出的预警，本质上是希望能推动联合国的讨论，以便在这种武器尚未被发明创造前就杜绝其使用。</p>
<p>他们认为，这种致命性自动化武器（LAWS）很可能会给人类带来预估以外的威胁。</p>
<blockquote class="com-insert-quote"> <div class="quote-hd">
<p>致命自动武器有可能成为第三次武器革命。一旦开发成功，它们将把冲突的规模带到前从未有的大，也会超过人类能认知的速度。它们将成为恐怖的武器，将成为暴君和恐怖分子残害无辜民众的武器，或者被黑客挟持用于预期之外的功用。我们没有足够的时间行动。一旦潘多拉的盒子被打开，就很难被关上。<br/></p> </div> <div class="quote-bd">
<p>116 位机器人、人工智能公司的创始人提交给联合国的“预警”信</p> </div> </blockquote>
<p>联合国从 2014 年 5 月开始<a href="http://www.un.org/apps/news/story.asp?NewsID=47794#.WZpP-XcjHMI" rel="nofollow">讨论</a>这种武器，也是在做预防的工作。前联合国秘书长<a href="http://www.un.org/apps/news/story.asp?NewsID=47794#.WZpP-XcjHMI" rel="nofollow">潘基文称</a>：“虽然自动武器系统尚未被部署，并且作为军用技术而言，它们的开发程度尚不清楚，这些问题的讨论必须立刻开始，不能等到这种技术被开发和扩散后再讨论。”</p>
<p>但有不少人认为，这种武器的诞生已经不远了。加州大学伯克利分校的机器科学和工程<a href="https://www.buzzfeed.com/sarahatopol/how-to-save-mankind-from-the-new-breed-of-killer-robots?utm_term=.yyEYAVodL#.qlWoLKWg7" rel="nofollow">教授 Stuart Russell 认为</a>，所需要的技术基础现在都已经具备，不需要另外的技术突破，问题在于会有多少资源被投入进去开发。<br/></p>
<p>这次的预警信是针对于联合国从去年开始筹备的讨论，进一步讨论对这种武器的禁止。</p>
<p>总的来说，致命性自动化武器（LAWS）<a href="https://www.buzzfeed.com/sarahatopol/how-to-save-mankind-from-the-new-breed-of-killer-robots?utm_term=.yyEYAVodL#.qlWoLKWg7" rel="nofollow">被认为可以分为 3 种</a>，一种是需要人类授权射击开火，也就是说人是攻击的一环；另一种是机器人可以自己执行攻击任务，但人类可以否决；最后一种是完全没有人为的干预，机器自动完成攻击任务。</p>
<p>不难理解军队对杀手机器人的期待：它不会累不会困不会受道德谴责，打完仗也不会因为战后创伤自杀造成负面影响。</p>
<p>联合国从去年开始筹备、讨论的是，哪几种武器应该被禁止。他们为此设立了一个政府专家组（GGE），专门用于讨论这项武器相关的话题，最终可能是起草一份应对计划。其中还涉及到一个问题是，不少国家的防御系统本质上接近于这种自动武器，这也加大了联合国讨论的难度。事实上，联合国比较成功的一次武器禁令还是在 1995 年。</p>
<p><br/></p>
<p>题图来自：Pixabay</p>

 <div class="embed-mask"><div class="embed-bd"><span class="triangle"/><div class="play"/></div><div class="embed-control"><span class="player"/><div class="bar"><span/></div><span class="sound iconfont icon-shengyin"/></div></div><p><img data-format="jpg" data-ratio="0.456667" class="lazyload lazylood" src="http://img.qdaily.com/uploads/20160725204735MvuXsg6iz3bhj7yi.jpg-w600" alt=""/></p><p class="lazylood">喜欢这篇文章？去 App 商店搜 <a href="http://m.qdaily.com/mobile/downloads/empty/2">好奇心日报</a> ，每天看点不一样的。</p></div></div>